{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/summerpurschke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import copy\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/summerpurschke/Desktop/ADS/ADS509(TextMining)/Mod2\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"/twitter/\"\n",
    "lyrics_folder = \"/lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        fdist = FreqDist(tokens)\n",
    "        print(f\"The {num_tokens} most common tokens are:\")\n",
    "        for token, frequency in fdist.most_common(num_tokens)[:5]:\n",
    "            print(f\"{token}: {frequency}\")\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The 13 most common tokens are:\n",
      "text: 3\n",
      "here: 2\n",
      "example: 2\n",
      "is: 1\n",
      "some: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion statements are a great way to check your own code for errors. These statements can help to identify and diagnose issues during development time, and can can serve as documentation for other programmers looking at your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adfaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lyrics_dictionary(folder_path):\n",
    "    lyrics_dict = {}\n",
    "\n",
    "    # Iterate over each item in the folder\n",
    "    for item_name in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item_name)\n",
    "\n",
    "        # Check if the item is a folder (artist folder)\n",
    "        if os.path.isdir(item_path):\n",
    "            artist_dict = {}\n",
    "\n",
    "            # Iterate over each file in the artist folder\n",
    "            for filename in os.listdir(item_path):\n",
    "                file_path = os.path.join(item_path, filename)\n",
    "\n",
    "                # Check if the item is a file (song file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, \"r\") as file:\n",
    "                        # Read the contents of the file\n",
    "                        lines = file.readlines()\n",
    "\n",
    "                        if lines:\n",
    "                            title = lines[0].strip()  # Assuming the first line contains the title\n",
    "                            lyrics = ''.join(lines[1:])  # Combine the remaining lines as the lyrics\n",
    "\n",
    "                            # Add the song lyrics to the artist's dictionary with the title as the inner key\n",
    "                            artist_dict[title] = lyrics\n",
    "\n",
    "            # Add the artist's dictionary to the main lyrics dictionary\n",
    "            lyrics_dict[item_name] = artist_dict\n",
    "\n",
    "    return lyrics_dict\n",
    "\n",
    "# Call the function with the lyrics folder path\n",
    "lyrics_dictionary = create_lyrics_dictionary(data_location + lyrics_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd3 in position 8: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e6c63e325934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtwitter_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd3 in position 8: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "twitter_files = os.listdir(data_location + twitter_folder)\n",
    "desc_files = [f for f in twitter_files if \"followers_data\" in f]\n",
    "twitter_data = defaultdict(list)\n",
    "for f in desc_files :\n",
    "    artist = f.split(\"_\")[0]\n",
    "        \n",
    "    with open(data_location + twitter_folder + f,'r', encoding='utf8') as infile :\n",
    "        next(infile)\n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line) == 7 :\n",
    "                twitter_data[artist].append(line[6])\n",
    "\n",
    "twitter_data = dict(twitter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for removing punctuation from dictionary \n",
    "def clean_text(text):\n",
    "    cleaned_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return cleaned_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d899dc91",
   "metadata": {},
   "source": [
    "Clean Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new dictionary that is empty - will add cleaned data as it's processed \n",
    "lyrics_dictionary_cleaned = {}\n",
    "\n",
    "# Fold to lowercase and populate lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary.items():\n",
    "    cleaned_songs = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        cleaned_lyrics = lyrics.lower()\n",
    "        cleaned_songs[song] = cleaned_lyrics\n",
    "    lyrics_dictionary_cleaned[artist] = cleaned_songs\n",
    "\n",
    "# Remove stopwords directly from lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    for song, lyrics in songs.items():\n",
    "        # Split the lyrics into individual words\n",
    "        words = lyrics.split()\n",
    "        # Remove stopwords from the list of words\n",
    "        cleaned_words = [word for word in words if word.lower() not in sw]\n",
    "        # Join the cleaned words back into a single string\n",
    "        cleaned_lyrics = \" \".join(cleaned_words)\n",
    "        # Update the lyrics in the lyrics_dictionary_cleaned\n",
    "        lyrics_dictionary_cleaned[artist][song] = cleaned_lyrics\n",
    "\n",
    "# Remove punctuation directly from lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    cleaned_songs = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        # Remove punctuation marks\n",
    "        cleaned_lyrics = lyrics.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        cleaned_songs[song] = cleaned_lyrics\n",
    "    lyrics_dictionary_cleaned[artist] = cleaned_songs\n",
    "\n",
    "# split at whitespace \n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    for song, lyrics in songs.items():\n",
    "        split_lyrics = lyrics.split()\n",
    "        lyrics_dictionary_cleaned[artist][song] = split_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of outer keys \n",
    "for artist, songs in lyrics_dictionary_cleaned.items():\n",
    "    word_list = []\n",
    "    for song, lyrics in songs.items():\n",
    "        word_list.extend(lyrics)\n",
    "    lyrics_dictionary_cleaned[artist] = word_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be99e8ca",
   "metadata": {},
   "source": [
    "Clean Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new dictionary that is empty - will add cleaned data as it's processed \n",
    "twitter_data_cleaned = {}\n",
    "\n",
    "# Fold to lowercase and populate the cleaned dictionary\n",
    "for artist, tweets in twitter_data.items():\n",
    "    cleaned_tweets = {}\n",
    "    for idx, words in enumerate(tweets):\n",
    "        cleaned_words = words.lower()\n",
    "        cleaned_tweets[f'tweet{idx+1}'] = cleaned_words\n",
    "    twitter_data_cleaned[artist] = cleaned_tweets\n",
    "\n",
    "# Remove stopwords directly from twitter_data_cleaned\n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    for tweet, words in tweets.items():\n",
    "        # Split the words into individual tokens\n",
    "        tokens = words.split()\n",
    "        # Remove stopwords from the list of tokens\n",
    "        cleaned_tokens = [token for token in tokens if token.lower() not in sw]\n",
    "        # Join the cleaned tokens back into a single string\n",
    "        cleaned_words = \" \".join(cleaned_tokens)\n",
    "        # Update the words in the twitter_data_cleaned\n",
    "        twitter_data_cleaned[artist][tweet] = cleaned_words\n",
    "\n",
    "# Remove punctuation directly from twitter_data_cleaned\n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    cleaned_tweets = {}\n",
    "    for tweet, words in tweets.items():\n",
    "        cleaned_words = ''.join(character for character in words if character not in string.punctuation)\n",
    "        cleaned_tweets[tweet] = cleaned_words\n",
    "    twitter_data_cleaned[artist] = cleaned_tweets\n",
    "\n",
    "# split at whitespace \n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    for tweet, words in tweets.items():\n",
    "        split_words = words.split()\n",
    "        twitter_data_cleaned[artist][tweet] = split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28affe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of outer keys\n",
    "for artist, tweets in twitter_data_cleaned.items():\n",
    "    word_list = []\n",
    "    for tweet, words in tweets.items():\n",
    "        word_list.extend(words)\n",
    "    twitter_data_cleaned[artist] = word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: If we left stopwords in the data, the top 5 words of each twitter account and lyrics database would likely be much more similar than when we removed them. \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: The artists have similar lexical diversity, which I expected knowing fairly little about both artists. I knew that the artists are similar in style and public stances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Token Lists for Lyrics \n",
    "cher_lyric_tokens= lyrics_dictionary_cleaned['cher']\n",
    "\n",
    "# call descriptive stats\n",
    "descriptive_stats(cher_lyric_tokens, num_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Token Lists for Lyrics \n",
    "robyn_lyric_tokens= lyrics_dictionary_cleaned['robyn']\n",
    "\n",
    "# call descriptive stats\n",
    "descriptive_stats(robyn_lyric_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbb860ce",
   "metadata": {},
   "source": [
    "Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Token Lists for Twitter Data\n",
    "cher_twitter_tokens= twitter_data_cleaned['cher']\n",
    "# Call descriptive stats on Twitter Data\n",
    "descriptive_stats(cher_twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b318846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Token Lists for Twitter Data\n",
    "robyn_twitter_tokens= twitter_data_cleaned['robynkonichiwa']\n",
    "\n",
    "# Call descriptive stats on Twitter Data\n",
    "descriptive_stats(robyn_twitter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists \n",
    "cher_tweets = twitter_data_cleaned['cher'] \n",
    "robyn_tweets = twitter_data_cleaned['robynkonichiwa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_emoji_counter = {}\n",
    "\n",
    "for item in cher_tweets:\n",
    "    emoji_list = emojis.get(item)\n",
    "    for emoji in emoji_list:\n",
    "        if emoji in cher_emoji_counter:\n",
    "            cher_emoji_counter[emoji] += 1\n",
    "        else:\n",
    "            cher_emoji_counter[emoji] = 1\n",
    "\n",
    "# Get the top 10 most common emojis\n",
    "top_10_emojis = sorted(cher_emoji_counter.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 most common emojis\n",
    "print(\"Top 10 most common emojis in Cher's Tweets:\")\n",
    "for emoji, count in top_10_emojis:\n",
    "    print(f\"{emoji}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6075722",
   "metadata": {},
   "outputs": [],
   "source": [
    "robyn_emoji_counter = {}\n",
    "for item in robyn_tweets:\n",
    "    emoji_list = emojis.get(item)\n",
    "    for emoji in emoji_list:\n",
    "        if emoji in robyn_emoji_counter:\n",
    "            robyn_emoji_counter[emoji] += 1\n",
    "        else:\n",
    "            robyn_emoji_counter[emoji] = 1\n",
    "\n",
    "# Get the top 10 most common emojis\n",
    "top_10_emojis = sorted(robyn_emoji_counter.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the top 10 most common emojis\n",
    "print(\"Top 10 most common emojis in Robyn's Tweets:\")\n",
    "for emoji, count in top_10_emojis:\n",
    "    print(f\"{emoji}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448577a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, tweets in twitter_data.items():\n",
    "    word_list = []\n",
    "    for tweet in tweets:\n",
    "        words = tweet.split()  # Split the tweet at whitespace\n",
    "        word_list.extend(words)\n",
    "    twitter_data[artist] = word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of hashtags\n",
    "cher_hashtags = [tweet.lower() for tweet in twitter_data['cher'] if tweet.lower().startswith('#')]\n",
    "\n",
    "robyn_hashtags = [tweet.lower() for tweet in twitter_data['robynkonichiwa'] if tweet.lower().startswith('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_10_hashtags(words):\n",
    "    hashtags_counter = Counter()\n",
    "    \n",
    "    # Iterate over each word in the list\n",
    "    for word in words:\n",
    "        # Check if the word starts with a '#' character\n",
    "        if word.startswith('#'):\n",
    "            # Remove any leading or trailing punctuation from the word\n",
    "            hashtag = word.strip()\n",
    "            # Update the counter with the hashtag\n",
    "            hashtags_counter[hashtag] += 1\n",
    "    \n",
    "    # Get the top 10 most common hashtags\n",
    "    top_10_hashtags = hashtags_counter.most_common(10)\n",
    "    \n",
    "    return top_10_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_top_hashtags = find_top_10_hashtags(cher_hashtags)\n",
    "\n",
    "print(\"Top 10 most common hashtags in Cher's Tweets:\")\n",
    "# Print the top 10 hashtags\n",
    "for hashtag, count in cher_top_hashtags:\n",
    "    print(f\"{hashtag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e094ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "robyn_top_hashtags = find_top_10_hashtags(robyn_hashtags)\n",
    "\n",
    "print(\"Top 10 most common hashtags in Robyn's Tweets:\")\n",
    "# Print the top 10 hashtags\n",
    "for hashtag, count in robyn_top_hashtags:\n",
    "    print(f\"{hashtag}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dictionary of just song titles per artist \n",
    "song_titles = {}\n",
    "for artist, songs in lyrics_dictionary.items():\n",
    "    song_titles[artist] = list(songs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e73900",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_titles_cleaned = {}\n",
    "\n",
    "# Iterate over each artist in the dictionary\n",
    "for artist, songs in song_titles.items():\n",
    "    # Create a list to store the cleaned song titles\n",
    "    cleaned_titles = []\n",
    "    \n",
    "    # Iterate over each song title for the current artist\n",
    "    for title in songs:\n",
    "        # Convert the title to lowercase\n",
    "        cleaned_title = title.lower()\n",
    "        \n",
    "        # Remove punctuation from the title\n",
    "        cleaned_title = \"\".join(char for char in cleaned_title if char not in string.punctuation)\n",
    "        \n",
    "        # Split the title by whitespace\n",
    "        words = cleaned_title.split()\n",
    "        \n",
    "        # Remove stop words from the title\n",
    "        cleaned_words = [word for word in words if word not in sw]\n",
    "        \n",
    "        # Extend the cleaned_titles list with the cleaned words\n",
    "        cleaned_titles.extend(cleaned_words)\n",
    "    \n",
    "    # Update the dictionary with the cleaned titles for the current artist\n",
    "    song_titles_cleaned[artist] = cleaned_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89153d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_words(word_list, num_words=5):\n",
    "    # Create a Counter object from the word list\n",
    "    word_counts = Counter(word_list)\n",
    "    \n",
    "    # Get the most common words and their frequencies\n",
    "    most_common_words = word_counts.most_common(num_words)\n",
    "    \n",
    "    return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cher Song titles most common words:\n",
    "cher_common_title_words = find_most_common_words(song_titles_cleaned['cher'], num_words=5)\n",
    "print('Five most common words in Cher Song Titles: ')\n",
    "print(cher_common_title_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ong titles most common words:\n",
    "robyn_common_title_words = find_most_common_words(song_titles_cleaned['robyn'], num_words=5)\n",
    "print('Five most common words in Robyn Song Titles: ')\n",
    "print(robyn_common_title_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d256c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new dictionary that is empty - will add cleaned data as it's processed \n",
    "song_lengths = {}\n",
    "\n",
    "# Fold to lowercase and populate lyrics_dictionary_cleaned\n",
    "for artist, songs in lyrics_dictionary.items():\n",
    "    cleaned_songs = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        cleaned_lyrics = lyrics.lower()\n",
    "        cleaned_songs[song] = cleaned_lyrics\n",
    "    song_lengths[artist] = cleaned_songs\n",
    "\n",
    "# Remove stopwords directly from lyrics_dictionary_cleaned\n",
    "for artist, songs in song_lengths .items():\n",
    "    for song, lyrics in songs.items():\n",
    "        # Split the lyrics into individual words\n",
    "        words = lyrics.split()\n",
    "        # Remove stopwords from the list of words\n",
    "        cleaned_words = [word for word in words if word.lower() not in sw]\n",
    "        # Join the cleaned words back into a single string\n",
    "        cleaned_lyrics = \" \".join(cleaned_words)\n",
    "        # Update the lyrics in the lyrics_dictionary_cleaned\n",
    "        song_lengths [artist][song] = cleaned_lyrics\n",
    "\n",
    "# Remove punctuation directly from lyrics_dictionary_cleaned\n",
    "for artist, songs in song_lengths.items():\n",
    "    cleaned_songs = {}\n",
    "    for song, lyrics in songs.items():\n",
    "        # Remove punctuation marks\n",
    "        cleaned_lyrics = lyrics.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        cleaned_songs[song] = cleaned_lyrics\n",
    "    song_lengths [artist] = cleaned_songs\n",
    "\n",
    "# split at whitespace \n",
    "for artist, songs in song_lengths.items():\n",
    "    for song, lyrics in songs.items():\n",
    "        split_lyrics = lyrics.split()\n",
    "        song_lengths [artist][song] = split_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace each key with the length of that key (number of tokens in the songs)\n",
    "# remove the extra \"\" inside of the ''  \n",
    "for artist, songs in lyrics_dictionary.items():\n",
    "    for song, words in songs.items():\n",
    "        song_cleaned = song.strip('\"')\n",
    "        lyrics_dictionary[artist][song_cleaned] = len(words)\n",
    "        del lyrics_dictionary[artist][song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_song_lengths = song_lengths['cher']\n",
    "cher_song_lengths = song_lengths['cher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "word_counts = Counter(cher_lyric_tokens)\n",
    "frequencies = list(word_counts.values())\n",
    "\n",
    "plt.hist(frequencies, bins= 5)\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Word Occurrences')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_histogram(cher_lyric_tokens, num_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
